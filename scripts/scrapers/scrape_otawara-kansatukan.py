import requests
from bs4 import BeautifulSoup
import re
import unicodedata
from datetime import datetime
from supabase import create_client, Client
from dotenv import load_dotenv
from playwright.sync_api import sync_playwright
import os
import json, os
from dotenv import load_dotenv
from datetime import datetime
from bs4 import BeautifulSoup, NavigableString, Tag
from bs4 import NavigableString, Tag

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
with open(os.path.join(BASE_DIR, "exclude_keywords.json"), "r", encoding="utf-8") as f:
    EXCLUDE_KEYWORDS = json.load(f)

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½ç½®ã‹ã‚‰ãƒ«ãƒ¼ãƒˆã® .env.test ã‚’å‚ç…§
dotenv_path = os.path.join(BASE_DIR, ".env.test")
load_dotenv(dotenv_path=dotenv_path)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

print("âœ… URL =", SUPABASE_URL)
print("âœ… KEY =", '[OK]' if SUPABASE_KEY else '[MISSING]')

MUSEUM_ID = "6b5f53e2-23b9-4ad4-9838-374c3beb1a4f"

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def wareki_to_seireki(text: str) -> str:
    def repl(m):
        era_year = int(m.group(1))
        month    = int(m.group(2))
        day      = int(m.group(3))
        seireki_year = 2018 + era_year
        return f"{seireki_year}å¹´{month}æœˆ{day}æ—¥"
    return re.sub(r"ä»¤å’Œ(\d{1,2})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", repl, text)

def clean_text(text):
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def remove_duplicate_sentences(text):
    seen = set()
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ\n])\s*', text)
    unique_sentences = []
    for s in sentences:
        norm = clean_text(s)
        if norm and norm not in seen:
            seen.add(norm)
            unique_sentences.append(s.strip())
    return " ".join(unique_sentences)

def parse_date_range_ht(text: str):
    # å’Œæš¦â†’è¥¿æš¦
    text = wareki_to_seireki(text)

    # â€•â€•â€• â‘  è¥¿æš¦ä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³ â€•â€•â€•
    parts = re.findall(r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥", text)
    if parts:
        # é–‹å§‹æ—¥ã®ã¿è¥¿æš¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å–å¾—
        y1, m1, d1 = map(int, parts[0])
        start = f"{y1}/{m1:02d}/{d1:02d}"

        # çµ‚äº†æ—¥ï¼šã‚‚ã—åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ï¼’ã¤ä»¥ä¸Šãƒ’ãƒƒãƒˆã™ã‚Œã°è¥¿æš¦ä»˜ãçµ‚äº†æ—¥ã‚’ä½¿ã†
        if len(parts) > 1:
            y2, m2, d2 = map(int, parts[1])
            end = f"{y2}/{m2:02d}/{d2:02d}"
        else:
            # â‘¢ æœˆæ—¥ã ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã—ã¦è£œå®Œ
            md = re.findall(r"(\d{1,2})æœˆ(\d{1,2})æ—¥", text)
            if len(md) > 1:
                # 2ã¤ã‚ã‚’çµ‚äº†æ—¥ã¨ã™ã‚‹
                m2, d2 = map(int, md[1])
                # å¹´æ¨æ¸¬ãƒ­ã‚¸ãƒƒã‚¯ã‚’æµç”¨
                today = datetime.now()
                def infer_year(month: int):
                    if today.month >= 10 and month <= 3:
                        return today.year + 1
                    return today.year
                y2 = infer_year(m2)
                end = f"{y2}/{m2:02d}/{d2:02d}"
            else:
                end = start

        return start, end

    # â€•â€•â€• â‘¡ å¹´æŒ‡å®šãªã—ï¼ˆæœˆæ—¥ã ã‘ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ â€•â€•â€•
    md = re.findall(r"(\d{1,2})æœˆ(\d{1,2})æ—¥", text)
    if md:
        today = datetime.now()
        current_year  = today.year

        def infer_year(month: int):
            # ãŸã¨ãˆã°ã€10æœˆä»¥é™ã«1ï½3æœˆãªã‚‰ç¿Œå¹´ã€ãã‚Œä»¥å¤–ã¯ä»Šå¹´
            if today.month >= 10 and month <= 3:
                return current_year + 1
            return current_year

        # é–‹å§‹æ—¥
        m1, d1 = map(int, md[0])
        y1 = infer_year(m1)
        start = f"{y1}/{m1:02d}/{d1:02d}"

        # çµ‚äº†æ—¥ï¼ˆã‚ã‚Œã°ï¼‰
        if len(md) > 1:
            m2, d2 = map(int, md[1])
            y2 = infer_year(m2)
            end = f"{y2}/{m2:02d}/{d2:02d}"
        else:
            end = start

        return start, end

    # â‘¢ ã©ã¡ã‚‰ã«ã‚‚ãƒãƒƒãƒã—ãªã‹ã£ãŸã‚‰ None
    return None, None

def fetch_events():
    events = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto("https://kansatukan.jp/event.html")
        page.wait_for_selector("h2")

        soup = BeautifulSoup(page.content(), "html.parser")

        for title_el in soup.find_all("h2"):
            title = clean_text(title_el.get_text())
            if not title or not any(tok in title for tok in ["ä¼ç”»å±•", "è‡ªç„¶è¦³å¯Ÿä¼š", "ã‚¹ãƒãƒƒãƒˆå±•"]):
                continue

            # â”€â”€ ãƒ–ãƒ­ãƒƒã‚¯ä¸¸ã”ã¨ä½œæˆ â”€â”€
            block_parts = []
            for sib in title_el.next_siblings:
                if isinstance(sib, Tag) and sib.name == "h2":
                    break
                if isinstance(sib, NavigableString):
                    block_parts.append(sib.strip())
                elif isinstance(sib, Tag):
                    block_parts.append(sib.get_text(separator=" ").strip())
            block = " ".join(block_parts)

            # â”€â”€ çµµæ‰‹ç´™æ•™å®¤ã ã‘ã¯ [ æ—¥æ™‚ ] ä»¥é™ã«çµã‚‹ â”€â”€
            if "çµµæ‰‹ç´™ã«æŒ‘æˆ¦2" in title:
                if "[ æ—¥æ™‚ ]" in block:
                    block = block.split("[ æ—¥æ™‚ ]", 1)[1]
            # â”€â”€ ç”³è¾¼ã¿ä»¥é™ã¯å¸¸ã«å‰Šé™¤ â”€â”€
            block = re.sub(r"\[\s*ç”³è¾¼ã¿[^\]]*\].*$", "", block, flags=re.MULTILINE)

            # â”€â”€ æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º â”€â”€
            # â‘  ç¯„å›²è¡¨è¨˜
            m = re.search(
                r"(?:ä»¤å’Œ\d{1,2}å¹´)?\d{1,2}æœˆ\d{1,2}æ—¥"
                r"[^0-9\n]{0,6}[ï½~][^0-9\n]{0,6}"
                r"(?:ä»¤å’Œ\d{1,2}å¹´)?\d{1,2}æœˆ\d{1,2}æ—¥",
                block
            )
            if not m:
                # â‘¡ å˜ä¸€æ—¥ä»˜
                m = re.search(r"(?:ä»¤å’Œ\d{1,2}å¹´)?\d{1,2}æœˆ\d{1,2}æ—¥", block)
            if not m:
                print(f"âš ï¸ æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹å¤±æ•— â†’ ã‚¹ã‚­ãƒƒãƒ—: {title}")
                continue
            raw = m.group(0)

            # â”€â”€ 3) ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ãƒ‘ãƒ¼ã‚¹ â”€â”€
            txt = clean_text(raw)
            txt = re.sub(r"[ï¼ˆ\(].*?[ï¼‰\)]", "", txt)
            print(f"[DEBUG final txt] {txt!r}")

            start_date, end_date = parse_date_range_ht(txt)
            if not start_date:
                print(f"âš ï¸ æ—¥ä»˜â†’è¥¿æš¦å¤‰æ›å¤±æ•— â†’ ã‚¹ã‚­ãƒƒãƒ—: {title}")
                continue

            # â”€â”€ 4) é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š â”€â”€
            if any(kw in title for kw in EXCLUDE_KEYWORDS):
                print(f"âš ï¸ é™¤å¤–ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º â†’ ã‚¹ã‚­ãƒƒãƒ—: {title}")
                continue

            # â”€â”€ 5) èª¬æ˜æ–‡æŠ½å‡º â”€â”€
            desc_parts = []
            for sib in title_el.next_siblings:
                if isinstance(sib, Tag) and sib.name == "h2":
                    break
                text = sib.get_text(separator=" ") if isinstance(sib, Tag) else str(sib)
                desc_parts.append(clean_text(text))
            description = remove_duplicate_sentences(" ".join(desc_parts))

            # â”€â”€ ã‚¤ãƒ™ãƒ³ãƒˆç™»éŒ²ãƒ‡ãƒ¼ã‚¿ä½œæˆ â”€â”€
            events.append({
                "title": title,
                "museum_id": MUSEUM_ID,
                "start_date": start_date,
                "end_date": end_date,
                "event_description": description,
                "event_url": page.url,
            })

        browser.close()

    print(f"ğŸ“¦ å…¨ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {len(events)}")
    return events

def save_to_supabase(events):
    for event in events:
        normalized_title = clean_text(event["title"])  # æ­£è¦åŒ–ã‚’ã“ã“ã§ã‚‚ç¢ºå®Ÿã«é©ç”¨
        event["title"] = normalized_title

        existing = supabase.table("events")\
            .select("id")\
            .eq("museum_id", event["museum_id"])\
            .eq("title", event["title"])\
            .eq("start_date", event["start_date"])\
            .limit(1)\
            .execute()
        
        if existing.data and len(existing.data) > 0:
            # UPDATE
            event_id = existing.data[0]["id"]
            result = supabase.table("events").update(event).eq("id", event_id).execute()
            action = "ğŸ”„ æ›´æ–°å®Œäº†"
        else:
            # INSERT
            result = supabase.table("events").insert(event).execute()
            action = "ğŸ†• æ–°è¦ç™»éŒ²"

        # çµæœå‡ºåŠ›ï¼ˆæˆåŠŸ or ã‚¨ãƒ©ãƒ¼å†…å®¹è¡¨ç¤ºï¼‰
        if hasattr(result, "data") and result.data:
            print(f"{action}: {event['title']}")
        elif hasattr(result, "status_code") and result.status_code >= 400:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {event['title']} (HTTP {result.status_code})")
        else:
            print(f"âš ï¸ ä¸æ˜ãªçŠ¶æ…‹: {event['title']}")

if __name__ == "__main__":
    events = fetch_events()
    print(f"ğŸ“¦ {len(events)} ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—")
    save_to_supabase(events)
