import requests
from bs4 import BeautifulSoup
import re
import unicodedata
from datetime import datetime
from supabase import create_client, Client
from dotenv import load_dotenv
import os
import json

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
with open(os.path.join(BASE_DIR, "exclude_keywords.json"), "r", encoding="utf-8") as f:
    EXCLUDE_KEYWORDS = json.load(f)

dotenv_path = os.path.join(BASE_DIR, ".env.test")
load_dotenv(dotenv_path=dotenv_path)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

print("âœ… URL =", SUPABASE_URL)
print("âœ… KEY =", '[OK]' if SUPABASE_KEY else '[MISSING]')

MUSEUM_ID = "5fc0a4d6-2c29-45f7-a9f5-390f943f5270"

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def clean_text(text):
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def remove_duplicate_sentences(text):
    seen = set()
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ\n])\s*', text)
    unique_sentences = []
    for s in sentences:
        norm = clean_text(s)
        if norm and norm not in seen:
            seen.add(norm)
            unique_sentences.append(s.strip())
    return " ".join(unique_sentences)

def parse_date_range(text):
    # ã¾ãšæ›œæ—¥ï¼ˆä¾‹: "æ—¥æ›œæ—¥"ï¼‰ã‚’å‰Šé™¤
    text = re.sub(r"\([^\)]*\)", "", text).strip()

    # æ—¥ä»˜ç¯„å›²ï¼ˆä¾‹: "5æœˆ3æ—¥~5æ—¥"ï¼‰ã‚’æŠ½å‡º
    match = re.findall(r"(\d{1,2})æœˆ(\d{1,2})æ—¥\s*~\s*(\d{1,2})æœˆ(\d{1,2})æ—¥", text)

    if match:
        today = datetime.now()
        current_year = today.year

        # é–‹å§‹æ—¥ã¨çµ‚äº†æ—¥ã‚’è¨­å®š
        start_month, start_day = map(int, match[0][0:2])
        end_month, end_day = map(int, match[0][2:4])

        start_year = current_year
        end_year = current_year

        start = f"{start_year}/{start_month:02d}/{start_day:02d}"
        end = f"{end_year}/{end_month:02d}/{end_day:02d}"

        print(f"è§£æã•ã‚ŒãŸæ—¥ä»˜ç¯„å›²: é–‹å§‹æ—¥ {start}, çµ‚äº†æ—¥ {end}")  # ãƒ‡ãƒãƒƒã‚°ç”¨

        return start, end

    # ç¯„å›²ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼ˆå˜ä¸€æ—¥ä»˜ã®è§£æãªã©ï¼‰
    match_single = re.findall(r"(\d{1,2})æœˆ(\d{1,2})æ—¥", text)
    if match_single:
        # å˜ä¸€æ—¥ä»˜ã®å ´åˆ
        start_month, start_day = map(int, match_single[0])
        today = datetime.now()
        current_year = today.year

        start = f"{current_year}/{start_month:02d}/{start_day:02d}"
        end = start  # å˜ä¸€ã®æ—¥ä»˜ã¨ã—ã¦çµ‚äº†æ—¥ã‚‚åŒã˜ã«è¨­å®š

        print(f"å˜ä¸€æ—¥ä»˜è§£æ: é–‹å§‹æ—¥ {start}, çµ‚äº†æ—¥ {end}")  # ãƒ‡ãƒãƒƒã‚°ç”¨

        return start, end

    # è¤‡æ•°æ—¥ä»˜ã®å ´åˆã€æœ€åˆã¨æœ€å¾Œã®æ—¥ä»˜ã‚’æŠ½å‡º
    match_multiple = re.findall(r"(\d{1,2})æœˆ(\d{1,2})æ—¥", text)
    if match_multiple:
        start_month, start_day = map(int, match_multiple[0])
        end_month, end_day = map(int, match_multiple[-1])

        today = datetime.now()
        current_year = today.year

        start = f"{current_year}/{start_month:02d}/{start_day:02d}"
        end = f"{current_year}/{end_month:02d}/{end_day:02d}"

        print(f"è¤‡æ•°æ—¥ä»˜è§£æ: é–‹å§‹æ—¥ {start}, çµ‚äº†æ—¥ {end}")  # ãƒ‡ãƒãƒƒã‚°ç”¨

        return start, end

    print(f"æ—¥ä»˜ç¯„å›²ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {text}")
    return None, None

def fetch_events():
    url = "https://www.city.tainai.niigata.jp/kurashi/kyoiku/bunka-sports/insect/kyousitsu/kyousitsu.html"
    res = requests.get(url)
    res.encoding = res.apparent_encoding
    soup = BeautifulSoup(res.text, "html.parser")

    events = []

    # ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’å–å¾—
    event_elements = soup.find_all("h3")  # h3ã‚¿ã‚°å†…ã«ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹
    print(f"ã‚¤ãƒ™ãƒ³ãƒˆãŒ {len(event_elements)} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")  # ãƒ‡ãƒãƒƒã‚°ç”¨

    for event in event_elements:
        title = clean_text(event.text)  # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
        print(f"ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«: {title}")  # ãƒ‡ãƒãƒƒã‚°ç”¨

        # ã‚¤ãƒ™ãƒ³ãƒˆæ—¥ä»˜ã‚’å–å¾—ï¼ˆh3ã‚¿ã‚°å†…ã«æ—¥ä»˜ãŒå«ã¾ã‚Œã¦ã„ã‚‹ï¼‰
        date_text = event.find_next("span", class_="txt_small")
        if date_text:
            date_range = clean_text(date_text.text)
            print(f"æ—¥ä»˜ç¯„å›²: {date_range}")  # ãƒ‡ãƒãƒƒã‚°ç”¨
            start_date, end_date = parse_date_range(date_range)

            description = ""  # èª¬æ˜æ–‡ãŒãªã„å ´åˆã‚‚ã‚ã‚‹ã®ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç©ºæ–‡å­—

            # ã‚¤ãƒ™ãƒ³ãƒˆã®è©³ç´°æƒ…å ±ã‚’å–å¾—
            details = event.find_next("p")  # æ¬¡ã®pã‚¿ã‚°ã«ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°ãŒå«ã¾ã‚Œã‚‹
            if details:
                description = clean_text(details.text)

            # é‡è¤‡é™¤å»
            description = remove_duplicate_sentences(description)

            # â‘  åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã§é™¤å¤–åˆ¤å®š
            if any(kw in title for kw in EXCLUDE_KEYWORDS):
                print(f"âš ï¸ é™¤å¤–ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º â†’ ã‚¹ã‚­ãƒƒãƒ—: {title}")
                continue

            if title and start_date:
                events.append({
                    "title": title,
                    "museum_id": MUSEUM_ID,
                    "start_date": start_date,
                    "end_date": end_date,
                    "event_description": description,
                    "event_url": url,
                })

    return events

def save_to_supabase(events):
    for event in events:
        normalized_title = clean_text(event["title"])
        existing = supabase.table("events")\
            .select("id")\
            .eq("museum_id", event["museum_id"])\
            .eq("title", event["title"])\
            .limit(1)\
            .execute()

        event["title"] = normalized_title  

        if existing.data and len(existing.data) > 0:
            event_id = existing.data[0]["id"]
            result = supabase.table("events").update(event).eq("id", event_id).execute()
            action = "ğŸ”„ æ›´æ–°å®Œäº†"
        else:
            result = supabase.table("events").insert(event).execute()
            action = "ğŸ†• æ–°è¦ç™»éŒ²"

        if hasattr(result, "data") and result.data:
            print(f"{action}: {event['title']}")
        elif hasattr(result, "status_code") and result.status_code >= 400:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {event['title']} (HTTP {result.status_code})")
        else:
            print(f"âš ï¸ ä¸æ˜ãªçŠ¶æ…‹: {event['title']}")

if __name__ == "__main__":
    events = fetch_events()
    print(f"ğŸ“¦ {len(events)} ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—")
    save_to_supabase(events)
