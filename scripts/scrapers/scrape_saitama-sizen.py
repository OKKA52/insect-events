import requests
from bs4 import BeautifulSoup
import re
import unicodedata
from datetime import datetime
from supabase import create_client, Client
from dotenv import load_dotenv
from playwright.sync_api import sync_playwright
import os
import json

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
with open(os.path.join(BASE_DIR, "exclude_keywords.json"), "r", encoding="utf-8") as f:
    EXCLUDE_KEYWORDS = json.load(f)

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½ç½®ã‹ã‚‰ãƒ«ãƒ¼ãƒˆã® .env.test ã‚’å‚ç…§
dotenv_path = os.path.join(BASE_DIR, ".env.test")
load_dotenv(dotenv_path=dotenv_path)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

print("âœ… URL =", SUPABASE_URL)
print("âœ… KEY =", '[OK]' if SUPABASE_KEY else '[MISSING]')

MUSEUM_ID = "a7164302-db2e-486b-837b-d2674e906455"  # å¿…è¦ãªIDã«å¤‰æ›´

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def clean_text(text):
    if not text:
        return ""
    text = text.strip()
    return text

# æ—¥æœ¬èªã®æœˆæ—¥ã‚’è¥¿æš¦ã®å½¢å¼ã«å¤‰æ›
def convert_japanese_date_to_standard(text):
    # æ—¥æœ¬èªã®æœˆæ—¥ã‚’ "5æœˆ10æ—¥(åœŸ)" ã®ã‚ˆã†ã«æ›¸ã‹ã‚ŒãŸå½¢å¼ã‚’ "2025/05/10" ã¸å¤‰æ›
    month_map = {
        '1æœˆ': '01', '2æœˆ': '02', '3æœˆ': '03', '4æœˆ': '04', '5æœˆ': '05', '6æœˆ': '06',
        '7æœˆ': '07', '8æœˆ': '08', '9æœˆ': '09', '10æœˆ': '10', '11æœˆ': '11', '12æœˆ': '12'
    }

    # "æœˆ" ã‚„ "æ—¥" ã®å¾Œã«ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
    # æ›œæ—¥ (ä¾‹: (åœŸ)) ã‚„ãã®ä»–ã®æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹ãŸã‚ã«æ­£è¦è¡¨ç¾ã‚’ä½¿ç”¨
    date_pattern = re.compile(r'(\d{1,2})æœˆ(\d{1,2})æ—¥.*')
    match = date_pattern.search(text)
    
    if match:
        month = month_map.get(f"{match.group(1)}æœˆ", "01")  # æœˆã®ãƒãƒƒãƒ”ãƒ³ã‚°
        day = match.group(2).zfill(2)  # æ—¥ä»˜ã‚’2æ¡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        return f"2025/{month}/{day}"  # ä¾‹: 2025/05/10
    return None

def parse_date_range(text):
    # ä¾‹: "5æœˆ10 æ—¥(åœŸ)" â†’ "2025/05/10"
    date = convert_japanese_date_to_standard(text)
    if date:
        return date, date  # é–‹å§‹æ—¥ã¨çµ‚äº†æ—¥ãŒåŒã˜å ´åˆã¯ä¸€åº¦ã®å€¤ã‚’è¿”ã™
    return None, None

def fetch_events():
    events = []
    url = "https://shizen.spec.ed.jp/ã‚¤ãƒ™ãƒ³ãƒˆ"  # åŸ¼ç‰çœŒç«‹è‡ªç„¶ã®åšç‰©é¤¨ã®URLã«å¤‰æ›´
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ãŒå…¥ã£ã¦ã„ã‚‹<div>ã‚’å–å¾—
    items = soup.find_all("div", class_="Box80-20 clear")
    if not items:
        print("ğŸ“­ ã‚¤ãƒ™ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒšãƒ¼ã‚¸çµ‚äº†ã€‚")
        return []

    for item in items:
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        title_el = item.select_one("p.Title")
        title = clean_text(title_el.text if title_el else "")

        # æœŸé–“ã‚’å–å¾—
        duration_el = item.select_one("p.Duration")
        duration = clean_text(duration_el.text if duration_el else "")

        # è©³ç´°ã‚’å–å¾—
        description_el = item.select_one("p:nth-of-type(3)")  # æœ€åˆã®è©³ç´°éƒ¨åˆ†ã‚’å–å¾—
        description = clean_text(description_el.text if description_el else "")

        # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if any(keyword in title for keyword in EXCLUDE_KEYWORDS):  # ãƒªã‚¹ãƒˆã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
            print(f"âš ï¸ é™¤å¤–ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º â†’ ã‚¹ã‚­ãƒƒãƒ—: {title}")
            continue  # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

        # æ—¥æœ¬èªã®æ—¥ä»˜å½¢å¼ã‚’æ¨™æº–ã®æ—¥ä»˜å½¢å¼ã«å¤‰æ›
        start_date, end_date = parse_date_range(duration)  # duration ã‚’å¤‰æ›

        if title and start_date:
            events.append({
                "title": title,
                "museum_id": MUSEUM_ID,
                "start_date": start_date,  # æ­£ã—ã„æ—¥ä»˜å½¢å¼ã‚’ä½¿ç”¨
                "end_date": end_date,  # åŒæ§˜ã«çµ‚äº†æ—¥ã‚‚æ­£ã—ã„å½¢å¼
                "event_description": description,
                "event_url": url,
            })

    return events

def save_to_supabase(events):
    for event in events:
        normalized_title = clean_text(event["title"])  # æ­£è¦åŒ–ã‚’ã“ã“ã§ã‚‚ç¢ºå®Ÿã«é©ç”¨
        event["title"] = normalized_title

        existing = supabase.table("events")\
            .select("id")\
            .eq("museum_id", event["museum_id"])\
            .eq("title", event["title"])\
            .eq("start_date", event["start_date"])\
            .limit(1)\
            .execute()

        if existing.data and len(existing.data) > 0:
            # UPDATE
            event_id = existing.data[0]["id"]
            result = supabase.table("events").update(event).eq("id", event_id).execute()
            action = "ğŸ”„ æ›´æ–°å®Œäº†"
        else:
            # INSERT
            result = supabase.table("events").insert(event).execute()
            action = "ğŸ†• æ–°è¦ç™»éŒ²"

        # çµæœå‡ºåŠ›ï¼ˆæˆåŠŸ or ã‚¨ãƒ©ãƒ¼å†…å®¹è¡¨ç¤ºï¼‰
        if hasattr(result, "data") and result.data:
            print(f"{action}: {event['title']}")
        elif hasattr(result, "status_code") and result.status_code >= 400:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {event['title']} (HTTP {result.status_code})")
        else:
            print(f"âš ï¸ ä¸æ˜ãªçŠ¶æ…‹: {event['title']}")

if __name__ == "__main__":
    events = fetch_events()
    print(f"ğŸ“¦ {len(events)} ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—")
    save_to_supabase(events)
