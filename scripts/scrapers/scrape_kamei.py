import requests
from bs4 import BeautifulSoup
import re
import unicodedata
from datetime import datetime
from supabase import create_client, Client
from dotenv import load_dotenv
import os
import json

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
with open(os.path.join(BASE_DIR, "exclude_keywords.json"), "r", encoding="utf-8") as f:
    EXCLUDE_KEYWORDS = json.load(f)

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½ç½®ã‹ã‚‰ãƒ«ãƒ¼ãƒˆã® .env.test ã‚’å‚ç…§
dotenv_path = os.path.join(BASE_DIR, ".env.test")
load_dotenv(dotenv_path=dotenv_path)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

print("âœ… URL =", SUPABASE_URL)
print("âœ… KEY =", '[OK]' if SUPABASE_KEY else '[MISSING]')

MUSEUM_ID = "850c696f-c867-453a-9bf5-b4b9ceec9bed"  # å¿…è¦ãªIDã«å¤‰æ›´

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def clean_text(text):
    if not text:
        return ""
    text = text.strip()
    return text

def convert_japanese_date_to_standard(text):
    # æœˆã®åå‰ã‚’ç•ªå·ã«å¤‰æ›
    month_map = {
        '1æœˆ': '01', '2æœˆ': '02', '3æœˆ': '03', '4æœˆ': '04', '5æœˆ': '05', '6æœˆ': '06',
        '7æœˆ': '07', '8æœˆ': '08', '9æœˆ': '09', '10æœˆ': '10', '11æœˆ': '11', '12æœˆ': '12'
    }

    # "å¹´", "æœˆ", "æ—¥" ã‚’å‰Šé™¤ã—ã¦å¹´æœˆæ—¥ã‚’æŠ½å‡º
    date_pattern = re.compile(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥')
    match = date_pattern.search(text)
    
    if match:
        year = match.group(1)
        month = match.group(2).zfill(2)  # æœˆãŒ1æ¡ã§ã‚‚2æ¡ã«å¤‰æ›
        day = match.group(3).zfill(2)   # æ—¥ä»˜ã‚’2æ¡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        result = f"{year}-{month}-{day}"  # ä¾‹: 2025-07-22
        print(f"ãƒ‡ãƒãƒƒã‚°: {text} â†’ {result}")  # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã®å¤‰æ›ã‚’ç¢ºèª
        return result
    return None

# æœŸé–“ã‚’å–å¾—ã—ã¦æ—¥ä»˜ç¯„å›²ã‚’å¤‰æ›ã™ã‚‹é–¢æ•°
def parse_date_range(text):
    # æ—¥ä»˜ç¯„å›²ãŒã€Œ - ã€ã§åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹å ´åˆã€æ—¥ä»˜ã®å‰å¾Œã®ç©ºç™½ã‚’å–ã‚Šé™¤ã„ã¦åˆ†å‰²
    dates = [date.strip() for date in text.split(' - ')]
    if len(dates) == 2:
        # æ—¥ä»˜ã®é †ç•ªãŒé€†è»¢ã—ã¦ã„ãªã„ã‹ç¢ºèª
        start_date = convert_japanese_date_to_standard(dates[0])
        end_date = convert_japanese_date_to_standard(dates[1])

        # ãƒ‡ãƒãƒƒã‚°: å–å¾—ã—ãŸæ—¥ä»˜ã‚’ç¢ºèª
        print(f"ãƒ‡ãƒãƒƒã‚°: å–å¾—ã—ãŸæ—¥ä»˜ç¯„å›²: {start_date} ï½ {end_date}")

        if start_date and end_date:
            # æ—¥ä»˜ã®é€†è»¢ã‚’æ¤œå‡º
            if start_date > end_date:
                print(f"âš ï¸ æ—¥ä»˜é †ç•ªãŒé€†è»¢ã—ã¾ã—ãŸ: {start_date} > {end_date}")
                start_date, end_date = end_date, start_date  # æ—¥ä»˜ã‚’äº¤æ›

            return start_date, end_date
        
    return None, None

def fetch_events():
    events = []
    url = "https://kameimuseum.or.jp/schedule/"  # äº€äº•åšç‰©é¤¨ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒšãƒ¼ã‚¸URL
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ãŒå…¥ã£ã¦ã„ã‚‹<div>ã‚’å–å¾—
    items = soup.find_all("div", class_="list_wrap")  # ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ãŒå«ã¾ã‚Œã‚‹ div ã‚’å¤‰æ›´
    if not items:
        print("ğŸ“­ ã‚¤ãƒ™ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒšãƒ¼ã‚¸çµ‚äº†ã€‚")
        return []

    for item in items:
        # "è¶"ãŒå«ã¾ã‚Œã‚‹ã‹ç¢ºèª
        category_el = item.select_one("span.cat_nenkan")
        if category_el and "è¶" not in category_el.text:
            continue  # "è¶"ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã€ã‚¹ã‚­ãƒƒãƒ—

        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        title_el = item.select_one("h4")
        title = clean_text(title_el.text if title_el else "")

        # æœŸé–“ã‚’å–å¾— (æ—¥ä»˜ç¯„å›²)
        duration_el = item.select_one("span.date_nenkan")
        if duration_el:
            duration = clean_text(duration_el.text)
            start_date, end_date = parse_date_range(duration)  # æ—¥ä»˜ç¯„å›²ã®å¤‰æ›

        # è©³ç´°ã‚’å–å¾—
        description_el = item.select_one("p")
        description = clean_text(description_el.text if description_el else "")

        # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if any(keyword in title for keyword in EXCLUDE_KEYWORDS):
            print(f"âš ï¸ é™¤å¤–ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º â†’ ã‚¹ã‚­ãƒƒãƒ—: {title}")
            continue

        if title and start_date:
            events.append({
                "title": title,
                "museum_id": MUSEUM_ID,
                "start_date": start_date,
                "end_date": end_date,
                "event_description": description,
                "event_url": url,
            })

    return events

def save_to_supabase(events):
    for event in events:
        normalized_title = clean_text(event["title"])  # æ­£è¦åŒ–ã‚’ã“ã“ã§ã‚‚ç¢ºå®Ÿã«é©ç”¨
        event["title"] = normalized_title

        existing = supabase.table("events") \
            .select("id") \
            .eq("museum_id", event["museum_id"]) \
            .eq("title", event["title"]) \
            .eq("start_date", event["start_date"]) \
            .limit(1) \
            .execute()

        if existing.data and len(existing.data) > 0:
            # UPDATE
            event_id = existing.data[0]["id"]
            result = supabase.table("events").update(event).eq("id", event_id).execute()
            action = "ğŸ”„ æ›´æ–°å®Œäº†"
        else:
            # INSERT
            result = supabase.table("events").insert(event).execute()
            action = "ğŸ†• æ–°è¦ç™»éŒ²"

        # çµæœå‡ºåŠ›ï¼ˆæˆåŠŸ or ã‚¨ãƒ©ãƒ¼å†…å®¹è¡¨ç¤ºï¼‰
        if hasattr(result, "data") and result.data:
            print(f"{action}: {event['title']}")
        elif hasattr(result, "status_code") and result.status_code >= 400:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {event['title']} (HTTP {result.status_code})")
        else:
            print(f"âš ï¸ ä¸æ˜ãªçŠ¶æ…‹: {event['title']}")

if __name__ == "__main__":
    events = fetch_events()
    print(f"ğŸ“¦ {len(events)} ä»¶ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—")
    save_to_supabase(events)
