import os
import re
import json
import unicodedata
from datetime import datetime
import requests
from bs4 import BeautifulSoup, Tag
from supabase import create_client, Client
from dotenv import load_dotenv

# â”€â”€ è¨­å®šèª­ã¿è¾¼ã¿ â”€â”€
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
with open(os.path.join(BASE_DIR, "exclude_keywords.json"), encoding="utf-8") as f:
    EXCLUDE_KEYWORDS = json.load(f)
load_dotenv(os.path.join(BASE_DIR, ".env.test"))
supabase: Client = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

MUSEUM_ID = "5a213ea6-704d-4401-b300-a4ecf5c9aab6"
LIST_URL   = "https://www.nat.museum.ibk.ed.jp/eventpage/daily.html"

def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", unicodedata.normalize("NFKC", s or "")).strip()

def fetch_html(url: str) -> str:
    r = requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
    r.raise_for_status()
    return r.text

def parse_date(raw: str) -> str:
    # YYYYå¹´MæœˆDæ—¥
    m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥", raw)
    if m:
        y,mo,d = map(int, m.groups())
    else:
        # M/D or YYYY/M/D
        m = re.search(r"(?:(\d{4})[\/ï¼])?(\d{1,2})[\/ï¼](\d{1,2})", raw)
        if not m:
            return None
        ys, ms, ds = m.groups()
        y = int(ys) if ys else datetime.now().year
        mo, d = int(ms), int(ds)
    return f"{y}/{mo:02d}/{d:02d}"

def fetch_events():
    print(f"ğŸŒ ä¸€è¦§ãƒšãƒ¼ã‚¸å–å¾—: {LIST_URL}")
    soup = BeautifulSoup(fetch_html(LIST_URL), "html.parser")
    events = []

    # <article onclick="location.href='...'" >
    for art in soup.find_all("article", onclick=True):
        # è©³ç´° URL ã‚’ onclick ã‹ã‚‰å–ã‚Šå‡ºã—
        m = re.search(r"location\.href=['\"](.+?)['\"]", art["onclick"])
        if not m:
            continue
        detail_url = requests.compat.urljoin(LIST_URL, m.group(1))

        # ã‚¿ã‚¤ãƒˆãƒ«
        h4 = art.find("h4")
        if not h4:
            continue
        title = clean_text(h4.get_text())
        if any(kw in title for kw in EXCLUDE_KEYWORDS) or title.startswith("å®šæœŸé–‹å‚¬"):
            continue

        # div.more å†…ã® li ã‹ã‚‰ã€Œã‚¤ãƒ™ãƒ³ãƒˆé–‹å‚¬æ—¥ã€
        date_li = art.select_one("div.more ul li:-soup-contains('ã‚¤ãƒ™ãƒ³ãƒˆé–‹å‚¬æ—¥')")
        if not date_li:
            print(f"âš ï¸ æ—¥ä»˜ li ãŒè¦‹ã¤ã‹ã‚‰ãšã‚¹ã‚­ãƒƒãƒ—: {title}")
            continue
        # <strong>ã‚¿ã‚°å†…ã«æ—¥ä»˜ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹ã®ã§ãã“ã‚’å–å¾—
        strong = date_li.find("strong")
        raw = clean_text(strong.get_text()) if strong else clean_text(date_li.get_text())
        date = parse_date(raw)
        if not date:
            print(f"âš ï¸ æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {title} raw={raw}")
            continue

        # èª¬æ˜æ–‡ï¼š<p> ã‚’ã¾ã¨ã‚ã‚‹
        desc = [ clean_text(p.get_text()) for p in art.find_all("p") ]

        print(f"ğŸ“ {title} â†’ {date} ï½ {date}")
        events.append({
            "title": title,
            "museum_id": MUSEUM_ID,
            "start_date": date,
            "end_date": date,
            "event_description": "\n".join(desc),
            "event_url": detail_url,
        })

    print(f"ğŸ“¦ å–å¾—ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {len(events)}")
    return events

def save_to_supabase(events):
    for ev in events:
        ev["title"] = clean_text(ev["title"])
        res = supabase.from_("events")\
            .select("id")\
            .eq("museum_id", ev["museum_id"])\
            .eq("title", ev["title"])\
            .eq("start_date", ev["start_date"])\
            .limit(1).execute()
        if res.data:
            supabase.from_("events").update(ev).eq("id", res.data[0]["id"]).execute()
            print(f"ğŸ”„ æ›´æ–°å®Œäº†: {ev['title']}")
        else:
            supabase.from_("events").insert(ev).execute()
            print(f"ğŸ†• æ–°è¦ç™»éŒ²: {ev['title']}")

if __name__ == "__main__":
    evs = fetch_events()
    save_to_supabase(evs)
